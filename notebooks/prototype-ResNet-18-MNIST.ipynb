{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56ca4df",
   "metadata": {},
   "source": [
    "# ResNet-18 with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b7d54",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced21f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6409d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0bf2ce",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea10735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"path\": \"../data\",\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": 10,\n",
    "        \"batch_size\": 512\n",
    "    },\n",
    "    \"model\": {\n",
    "        \n",
    "    },\n",
    "    \"training\":{\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"epochs\": 5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e549f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd213911",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7605d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for MNIST \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # specific for MNIST\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abb4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = config['data']['path'],\n",
    "    train = True,\n",
    "    transform = transform,\n",
    "    download = False\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = config['data']['path'],\n",
    "    train = False,\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f2e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = config['data']['batch_size'],\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = config['data']['batch_size'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78d76f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b43f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "    \n",
    "#     def __init__(self, in_channels, channels, stride=1):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         self.expansion = 1\n",
    "#         self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(channels)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != self.expansion*channels:\n",
    "#             # modify shortcut for correct channel output\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, self.expansion*channels, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion * channels)\n",
    "#             )\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = nn.ReLU()(out)\n",
    "#         return out\n",
    "    \n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, num_blocks, in_channels, num_classes):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.in_channels = in_channels\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.in_channels = 64 # modify as it after first channel\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "#         self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        \n",
    "#     def _make_layer(self, block, channels, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "        \n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_channels, channels, stride))\n",
    "#             self.in_channels = channels * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = nn.AdaptiveAvgPool2d(1)(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.linear(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried new architecture for ResNet-18\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # because MNIST is already 1x1 here:\n",
    "        # disable avg pooling\n",
    "        #x = self.avgpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "def resnet18(num_classes):\n",
    "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
    "    model = ResNet(block=BasicBlock, \n",
    "                   layers=[2, 2, 2, 2],\n",
    "                   num_classes=config['data']['num_classes'],\n",
    "                   grayscale=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f4edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet(\n",
    "#     BasicBlock, \n",
    "#     num_blocks = [2, 2, 2, 2], \n",
    "#     in_channels = config['data']['in_channels'],\n",
    "#     num_classes = config['data']['num_classes']\n",
    "# ).to(device)\n",
    "model = resnet18(10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871830c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c358a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for training\n",
    "def calculate_acc(logits, labels):\n",
    "    \"\"\"\n",
    "    Given logits and correct labels:\n",
    "    Return number of corrections and accuracy\n",
    "    \"\"\"\n",
    "    pred = logits.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().detach().cpu().item()\n",
    "    return correct, 100 * correct / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82854e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if config['training']['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "elif config['training']['topmizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be9914a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [loss:0.12, acc:96.83]: 100%|█████████████████████████████████| 5/5 [01:57<00:00, 23.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Assitant List for showing live loss/acc\n",
    "live_losses = []\n",
    "live_accs = []\n",
    "\n",
    "epoch_describer = tqdm(range(config['training']['epochs']), desc=f\"Epoch {1}\", ncols=100)\n",
    "\n",
    "training_losses = []\n",
    "training_accs = []\n",
    "validation_losses = []\n",
    "validation_accs = []\n",
    "    \n",
    "for epoch in epoch_describer:\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    training_corr = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct, accuracy = calculate_acc(outputs, labels)\n",
    "        \n",
    "        training_corr += correct\n",
    "        training_loss += loss.detach().cpu().item()\n",
    "        \n",
    "        live_losses.append(loss.detach().cpu().item())\n",
    "        live_accs.append(accuracy)\n",
    "        if len(live_losses) > 100:\n",
    "            live_losses.pop(0)\n",
    "            live_accs.pop(0)\n",
    "            \n",
    "        # update \n",
    "        epoch_describer.\\\n",
    "        set_description(f\"Epoch {epoch+1} [loss:{np.mean(live_losses):.2f}, acc:{np.mean(live_accs):.2f}]\")\n",
    "        \n",
    "    # record training data per epoch\n",
    "    training_losses.append(training_loss / len(train_loader))\n",
    "    training_accs.append(training_corr / len(train_loader.dataset))\n",
    "        \n",
    "    # validate the result\n",
    "    valid_loss = 0.0\n",
    "    valid_corr = 0.0\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs,_ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct, accuracy = calculate_acc(outputs, labels)\n",
    "            \n",
    "            valid_loss += loss.detach().cpu().item()\n",
    "            valid_corr += correct\n",
    "            \n",
    "    validation_losses.append(valid_loss / len(test_loader))\n",
    "    validation_accs.append(valid_corr / len(test_loader.dataset))                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/005 | Batch 0000/0118 | Cost: 2.3367\n",
      "Epoch: 001/005 | Batch 0050/0118 | Cost: 0.2200\n",
      "Epoch: 001/005 | Batch 0100/0118 | Cost: 0.1870\n",
      "Epoch: 001/005 | Train: 96.558%\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 002/005 | Batch 0000/0118 | Cost: 0.1186\n",
      "Epoch: 002/005 | Batch 0050/0118 | Cost: 0.1436\n",
      "Epoch: 002/005 | Batch 0100/0118 | Cost: 0.0967\n",
      "Epoch: 002/005 | Train: 97.928%\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 003/005 | Batch 0000/0118 | Cost: 0.1070\n",
      "Epoch: 003/005 | Batch 0050/0118 | Cost: 0.0504\n",
      "Epoch: 003/005 | Batch 0100/0118 | Cost: 0.0873\n",
      "Epoch: 003/005 | Train: 98.432%\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 004/005 | Batch 0000/0118 | Cost: 0.0550\n",
      "Epoch: 004/005 | Batch 0050/0118 | Cost: 0.0547\n",
      "Epoch: 004/005 | Batch 0100/0118 | Cost: 0.0427\n",
      "Epoch: 004/005 | Train: 99.228%\n",
      "Time elapsed: 1.92 min\n",
      "Epoch: 005/005 | Batch 0000/0118 | Cost: 0.0258\n",
      "Epoch: 005/005 | Batch 0050/0118 | Cost: 0.0497\n",
      "Epoch: 005/005 | Batch 0100/0118 | Cost: 0.0422\n",
      "Epoch: 005/005 | Train: 99.533%\n",
      "Time elapsed: 2.44 min\n",
      "Total Training Time: 2.44 min\n"
     ]
    }
   ],
   "source": [
    "# train alternate ResNet-18\n",
    "import time\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 28*28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = True\n",
    "\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb89c190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6048833333333333,\n",
       " 0.90955,\n",
       " 0.9425333333333333,\n",
       " 0.9578166666666666,\n",
       " 0.9680833333333333]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45d70d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8809, 0.9307, 0.9473, 0.9555, 0.9611]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code imported form convrfm/cnfa_verification/pretrained_conv_nfa.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from functorch import jacrev, vmap\n",
    "from torch.nn.functional import pad\n",
    "from numpy.linalg import eig\n",
    "from copy import deepcopy\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "SEED = 2323\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def patchify(x, patch_size, stride_size, pad_type='zeros'):\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "\n",
    "    pad_1 = (q1-1)//2\n",
    "    pad_2 = (q2-1)//2\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "\n",
    "    patches = x.unfold(2, q1, s1).unfold(3, q2, s2)\n",
    "    patches = patches.transpose(1, 3).transpose(1, 2)\n",
    "    return patches\n",
    "\n",
    "\n",
    "class PatchConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer\n",
    "\n",
    "    def forward(self, patches):\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PatchBasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, block_layer, downsample=False):\n",
    "        super().__init__()\n",
    "        self.layer = block_layer\n",
    "        self.downsample = downsample\n",
    "\n",
    "    # x is patches instead of images\n",
    "    def forward(self, X):\n",
    "        x, y = X\n",
    "        ops = self.layer\n",
    "        _, _, _, _, q, s = y.shape\n",
    "        z = y[:, :, :, :, (q-1)//2, (s-1)//2]\n",
    "        z = z.transpose(1, 3).transpose(2, 3)\n",
    "\n",
    "        s1, s2 = ops.conv1.layer.stride\n",
    "        x = x[:, ::s1, ::s2, :, :, :]\n",
    "        o = ops.conv1(x).contiguous()\n",
    "        o = ops.bn1(o)\n",
    "        o = ops.relu(o)\n",
    "        o = ops.conv2(o)\n",
    "        o = ops.bn2(o)\n",
    "\n",
    "        if self.downsample:\n",
    "            z = ops.downsample(z)\n",
    "        o += z\n",
    "        o = ops.relu(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "class PatchBottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self, block_layer, downsample=False):\n",
    "        super().__init__()\n",
    "        self.layer = block_layer\n",
    "        self.downsample = downsample\n",
    "\n",
    "    # x is patches instead of images\n",
    "    def forward(self, X):\n",
    "        x, y = X\n",
    "        ops = self.layer\n",
    "        _, _, _, _, q, s = x.shape\n",
    "\n",
    "        z = y[:, :, :, :, (q-1)//2, (s-1)//2]\n",
    "        z = z.transpose(1, 3).transpose(2, 3)\n",
    "\n",
    "        s1, s2 = ops.conv1.layer.stride\n",
    "        x = x[:, ::s1, ::s2, :, :, :]\n",
    "        o = ops.conv1(x).contiguous()\n",
    "        o = ops.bn1(o)\n",
    "        o = ops.relu(o)\n",
    "\n",
    "        o = ops.conv2(o)\n",
    "        o = ops.bn2(o)\n",
    "        o = ops.relu(o)\n",
    "\n",
    "        o = ops.conv3(o)\n",
    "        o = ops.bn3(o)\n",
    "\n",
    "        if self.downsample:\n",
    "            z = ops.downsample(z)\n",
    "        o += z\n",
    "        o = ops.relu(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "def get_jacobian(net, data, c_idx=0, chunk=100):\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        def single_net(x):\n",
    "            o = net(x)[:,c_idx*chunk:(c_idx+1)*chunk]\n",
    "            return o\n",
    "        return vmap(jacrev(single_net))(data)\n",
    "\n",
    "\n",
    "def egop(model, X):\n",
    "    ajop = 0\n",
    "    c = 1000\n",
    "    chunk_idxs = 100\n",
    "    chunk = c // chunk_idxs\n",
    "    for i in range(chunk_idxs):\n",
    "        J = get_jacobian(model, X, c_idx=i, chunk=chunk)[0]\n",
    "        J = J[0, 0].transpose(0, 1)\n",
    "        # n is number of images\n",
    "        # c is number of channels\n",
    "        # w, h give number of total patches\n",
    "        n, c, w, h, _, _, _ = J.shape\n",
    "        J = J.transpose(1, 3).transpose(1, 2)\n",
    "\n",
    "        grads = J.reshape(n*w*h, c, -1)\n",
    "        ajop += torch.einsum('ncd, ncD -> dD', grads, grads)\n",
    "    return ajop\n",
    "\n",
    "\n",
    "def load_nn(net, init_net,\n",
    "            block_idx=0):\n",
    "\n",
    "    patchnet = deepcopy(net)\n",
    "    l_idx = block_idx\n",
    "    layer_idx = 0\n",
    "    subnet = net[:l_idx]\n",
    "    for m in subnet.children():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            layer_idx += 1\n",
    "        # For Resnet18, 34\n",
    "        elif isinstance(m, torchvision.models.resnet.BasicBlock):\n",
    "        # For ResNet50 -> 152\n",
    "        #elif isinstance(m, torchvision.models.resnet.Bottleneck):\n",
    "            modules = [mod for mod in m.modules() if not isinstance(mod, nn.Sequential)]\n",
    "            for mod in modules:\n",
    "                if isinstance(mod, nn.Conv2d):\n",
    "                    layer_idx += 1\n",
    "\n",
    "    patchnet = patchnet[l_idx:]\n",
    "    if block_idx == 0:\n",
    "        patchnet[0] = PatchConvLayer(patchnet[0])\n",
    "    else:\n",
    "\n",
    "        if patchnet[0].downsample is not None:\n",
    "            downsample = True\n",
    "        else:\n",
    "            downsample = False\n",
    "\n",
    "        # For Resnet18, 34\n",
    "        patchnet[0].conv1 = PatchConvLayer(patchnet[0].conv1)\n",
    "        patchnet[0] = PatchBasicBlock(patchnet[0], downsample=downsample)\n",
    "\n",
    "        # For Resnet50 -> 152\n",
    "        #patchnet[0].conv1 = PatchConvLayer(patchnet[0].conv1)\n",
    "        #patchnet[0] = PatchBottleneck(patchnet[0], downsample=downsample)\n",
    "\n",
    "    count = -1\n",
    "    for idx, p in enumerate(net.parameters()):\n",
    "        if len(p.shape) > 1:\n",
    "            count += 1\n",
    "        if count == layer_idx:\n",
    "            M = p.data\n",
    "            _, ki, q, s = M.shape\n",
    "\n",
    "            M0 = [p for p in init_net.parameters()][idx].data\n",
    "\n",
    "            M = M.reshape(-1, ki*q*s)\n",
    "            M = torch.einsum('nd, nD -> dD', M, M)\n",
    "\n",
    "            M0 = M0.reshape(-1, ki*q*s)\n",
    "            M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "            break\n",
    "\n",
    "    return net, patchnet, M, M0, l_idx, (q, s)\n",
    "\n",
    "\n",
    "def get_grads(net, patchnet, trainloader,\n",
    "              kernel=(3,3),\n",
    "              layer_idx=0):\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    patchnet.eval()\n",
    "    patchnet.cuda()\n",
    "    M = 0\n",
    "    q,s = kernel\n",
    "\n",
    "    # Num images for taking AGOP (set to >100 for deeper layers)\n",
    "    MAX_NUM_IMGS = 10\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()\n",
    "            imgs = net[:layer_idx](imgs).cpu()\n",
    "        patches = patchify(imgs, (q, s), (1, 1))\n",
    "        p_copy = deepcopy(patches)\n",
    "        patches = patches.cuda()\n",
    "        p_copy = p_copy.cuda()\n",
    "\n",
    "        M += egop(patchnet, [patches.unsqueeze(0), p_copy.unsqueeze(0)]).cpu()\n",
    "        del imgs, patches\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break\n",
    "    net.cpu()\n",
    "    patchnet.cpu()\n",
    "    return M\n",
    "\n",
    "\n",
    "def min_max(M):\n",
    "    return (M - M.min()) / (M.max() - M.min())\n",
    "\n",
    "\n",
    "def correlation(M1, M2):\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "    M1 = M1.to(device)\n",
    "    M2.to(device)\n",
    "    \n",
    "    return torch.sum(M1 * M2) / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def verify_NFA(net, init_net, trainloader, layer_idx=0):\n",
    "\n",
    "    net, patchnet, M, M0, l_idx, (q, s) = load_nn(net,\n",
    "                                                  init_net,\n",
    "                                                  block_idx=layer_idx)\n",
    "  \n",
    "    i_val = correlation(M0, M)\n",
    "    print(\"Correlation between Initial and Trained CNFM: \", i_val)\n",
    "\n",
    "    G = get_grads(net, patchnet, trainloader, kernel=(q, s), layer_idx=l_idx)\n",
    "    \n",
    "    r_val = correlation(M, G)\n",
    "\n",
    "    print(\"Correlation between Trained CNFM and AGOP: \", r_val)\n",
    "    print(\"Final: \", i_val, r_val)\n",
    "\n",
    "    return i_val.data.numpy(), r_val.data.numpy()\n",
    "\n",
    "\n",
    "def subroutine_unroll_net(net):\n",
    "    modules = list(net.children())\n",
    "    unrolled = []\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Sequential):\n",
    "            unrolled += subroutine_unroll_net(m)\n",
    "        else:\n",
    "            unrolled.append(m)\n",
    "    return unrolled\n",
    "\n",
    "\n",
    "def unroll_net(net):\n",
    "    modules = subroutine_unroll_net(net)[:-1]\n",
    "    modules += [nn.Flatten(), list(net.children())[-1]]\n",
    "    net = nn.Sequential(*modules)\n",
    "    return net\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Initial and Trained CNFM:  tensor(0.0998, device='cuda:0')\n",
      "Computing GOP for sample 0 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\30585\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\deprecated.py:80: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.jacrev is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.jacrev instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('jacrev')\n",
      "C:\\Users\\30585\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\30585\\Desktop\\DSC180A-Project1\\notebooks\\prototype-ResNet-18-MNIST.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     dataset \u001b[39m=\u001b[39m train_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m idxs:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     i_val, r_val \u001b[39m=\u001b[39m verify_NFA(net, init_net, train_loader, layer_idx\u001b[39m=\u001b[39;49midx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i_val) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(r_val), file\u001b[39m=\u001b[39moutf, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\30585\\Desktop\\DSC180A-Project1\\notebooks\\prototype-ResNet-18-MNIST.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m i_val \u001b[39m=\u001b[39m correlation(M0, M)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCorrelation between Initial and Trained CNFM: \u001b[39m\u001b[39m\"\u001b[39m, i_val)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m G \u001b[39m=\u001b[39m get_grads(net, patchnet, trainloader, kernel\u001b[39m=\u001b[39;49m(q, s), layer_idx\u001b[39m=\u001b[39;49ml_idx)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m r_val \u001b[39m=\u001b[39m correlation(M, G)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCorrelation between Trained CNFM and AGOP: \u001b[39m\u001b[39m\"\u001b[39m, r_val)\n",
      "\u001b[1;32mc:\\Users\\30585\\Desktop\\DSC180A-Project1\\notebooks\\prototype-ResNet-18-MNIST.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m patches \u001b[39m=\u001b[39m patches\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m p_copy \u001b[39m=\u001b[39m p_copy\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m M \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m egop(patchnet, [patches\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), p_copy\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)])\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m \u001b[39mdel\u001b[39;00m imgs, patches\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[1;32mc:\\Users\\30585\\Desktop\\DSC180A-Project1\\notebooks\\prototype-ResNet-18-MNIST.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m chunk \u001b[39m=\u001b[39m c \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m chunk_idxs\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(chunk_idxs):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     J \u001b[39m=\u001b[39m get_jacobian(model, X, c_idx\u001b[39m=\u001b[39;49mi, chunk\u001b[39m=\u001b[39;49mchunk)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     J \u001b[39m=\u001b[39m J[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39m# n is number of images\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m     \u001b[39m# c is number of channels\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m     \u001b[39m# w, h give number of total patches\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\30585\\Desktop\\DSC180A-Project1\\notebooks\\prototype-ResNet-18-MNIST.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     o \u001b[39m=\u001b[39m net(x)[:,c_idx\u001b[39m*\u001b[39mchunk:(c_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mchunk]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m o\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mreturn\u001b[39;00m vmap(jacrev(single_net))(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\vmap.py:434\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[0;32m    431\u001b[0m                          args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    433\u001b[0m \u001b[39m# If chunk_size is not specified.\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[0;32m    435\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    436\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\vmap.py:619\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[1;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[1;32m--> 619\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39mbatched_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[0;32m    621\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\eager_transforms.py:489\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper_fn\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m    488\u001b[0m     error_if_complex(\u001b[39m\"\u001b[39m\u001b[39mjacrev\u001b[39m\u001b[39m\"\u001b[39m, args, is_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 489\u001b[0m     vjp_out \u001b[39m=\u001b[39m _vjp_with_argnums(func, \u001b[39m*\u001b[39;49margs, argnums\u001b[39m=\u001b[39;49margnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)\n\u001b[0;32m    490\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[0;32m    491\u001b[0m         output, vjp_fn, aux \u001b[39m=\u001b[39m vjp_out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_functorch\\eager_transforms.py:291\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[1;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[0;32m    289\u001b[0m     diff_primals \u001b[39m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    290\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39mlevel), diff_primals)\n\u001b[1;32m--> 291\u001b[0m primals_out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mprimals)\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(primals_out, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(primals_out) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n",
      "\u001b[1;32mc:\\Users\\30585\\Desktop\\DSC180A-Project1\\notebooks\\prototype-ResNet-18-MNIST.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_net\u001b[39m(x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     o \u001b[39m=\u001b[39m net(x)[:,c_idx\u001b[39m*\u001b[39mchunk:(c_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mchunk]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/30585/Desktop/DSC180A-Project1/notebooks/prototype-ResNet-18-MNIST.ipynb#X33sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m o\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\pooling.py:639\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mavg_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    640\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount_include_pad, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdivisor_override)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small"
     ]
    }
   ],
   "source": [
    "# TODO: Fix error\n",
    "idxs = list(range(4, 12))\n",
    "\n",
    "fname = 'test.csv'\n",
    "outf = open(fname, 'w')\n",
    "\n",
    "net = model\n",
    "init_net = resnet18(10)\n",
    "\n",
    "net = unroll_net(net)\n",
    "init_net = unroll_net(init_net)\n",
    "\n",
    "# Set path to imagenet data\n",
    "\n",
    "# Batch size should be 1 to avoid issues with grads for skip connections\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # specific for MNIST\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = config['data']['path'],\n",
    "    train = True,\n",
    "    transform = transform,\n",
    "    download = False\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = config['data']['batch_size'],\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "for idx in idxs:\n",
    "    i_val, r_val = verify_NFA(net, init_net, train_loader, layer_idx=idx)\n",
    "    print(\"Layer \" + str(idx+1) + ',' + str(i_val) + ',' + str(r_val), file=outf, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([64, 1, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([10, 512])\n",
      "fc.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = load_nn(net,init_net,0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egop(p,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
